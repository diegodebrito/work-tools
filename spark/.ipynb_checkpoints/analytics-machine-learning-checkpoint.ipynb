{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib Overview (24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T13:56:40.476084Z",
     "start_time": "2020-05-26T13:56:40.459129Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# local meansthat driver and executers run on your individual\n",
    "# computer instead of a cluster. \n",
    "spark = SparkSession.builder.master(\"local\").appName(\"practice\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features passed to a ml model must be Double in Spark. We can have dense or sparse vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T13:59:24.916555Z",
     "start_time": "2020-05-26T13:59:24.909601Z"
    }
   },
   "outputs": [],
   "source": [
    "# pyspark.ml refers to the modern MLlib package, used with the structured API\n",
    "# don't use pyspark.mllib (this one is used with RDDs and is older)\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "denseVec = Vectors.dense(1.0, 2.0, 3.0)\n",
    "size = 3\n",
    "idx = [1, 2] # locations of non-zero elements in vector\n",
    "values = [2.0, 3.0]\n",
    "sparseVec = Vectors.sparse(size, idx, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T14:18:14.258602Z",
     "start_time": "2020-05-26T14:18:13.726657Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../Spark-The-Definitive-Guide/data/simple-ml\"\n",
    "df = spark.read.json(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T14:20:14.203507Z",
     "start_time": "2020-05-26T14:20:14.115966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green|good|    15| 38.97187133755819|\n",
      "|green|good|    12|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic transformations syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T15:00:12.505315Z",
     "start_time": "2020-05-26T15:00:12.272806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|color| lab|value1|            value2|            features|label|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "supervised = RFormula(formula=\"lab~ . + color:value1 + color:value2\")\n",
    "# in this case, we need to use fit\n",
    "fittedRF = supervised.fit(df)\n",
    "preparedDF = fittedRF.transform(df)\n",
    "preparedDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**creating a test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T15:01:07.473400Z",
     "start_time": "2020-05-26T15:01:07.446404Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = preparedDF.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**basic model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T15:04:17.025863Z",
     "start_time": "2020-05-26T15:04:16.458059Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "# fitting is eager and is performed immediately\n",
    "fittedLR = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T15:07:13.932775Z",
     "start_time": "2020-05-26T15:07:13.929812Z"
    }
   },
   "outputs": [],
   "source": [
    "## help for every algorithm on MLlib\n",
    "#print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T15:07:57.674126Z",
     "start_time": "2020-05-26T15:07:57.587588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fittedLR.transform(train).select(\"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T15:42:59.584681Z",
     "start_time": "2020-05-26T15:42:59.563810Z"
    }
   },
   "outputs": [],
   "source": [
    "# split of the original dataset this time\n",
    "train, test = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# stages for the pipeline \n",
    "rForm = RFormula() # open so we can test different configurations\n",
    "lr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "\n",
    "# creating the pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stages = [rForm, lr]\n",
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a parameter grid using the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T15:48:34.745406Z",
     "start_time": "2020-05-26T15:48:34.740449Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "params = ParamGridBuilder()\\\n",
    "    .addGrid(rForm.formula, [\"lab ~ . + color:value1\",\n",
    "                             \"lab ~ . + color:value1 + color:value2\"])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .addGrid(lr.regParam, [0.1, 2.0])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining evalutation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T15:56:00.771325Z",
     "start_time": "2020-05-26T15:56:00.762360Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    "    .setMetricName(\"areaUnderROC\")\\\n",
    "    .setRawPredictionCol(\"prediction\")\\\n",
    "    .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation strategy (we use a simple split in this case, but cross validation is also an option):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T15:56:01.640518Z",
     "start_time": "2020-05-26T15:56:01.635735Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "tvs = TrainValidationSplit()\\\n",
    "    .setTrainRatio(0.75)\\\n",
    "    .setEstimatorParamMaps(params)\\\n",
    "    .setEstimator(pipeline)\\\n",
    "    .setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T16:01:41.235296Z",
     "start_time": "2020-05-26T16:01:32.972452Z"
    }
   },
   "outputs": [],
   "source": [
    "tvsFitted = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T16:01:55.134815Z",
     "start_time": "2020-05-26T16:01:55.005009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8913043478260869"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(tvsFitted.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and FE (25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T20:53:29.742244Z",
     "start_time": "2020-06-04T20:53:17.966744Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# local meansthat driver and executers run on your individual\n",
    "# computer instead of a cluster. \n",
    "spark = SparkSession.builder.master(\"local\").appName(\"practice\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:playground]",
   "language": "python",
   "name": "conda-env-playground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
