{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T18:50:04.795575Z",
     "start_time": "2020-05-12T18:50:04.786596Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My notes from the book Spark - The Definitive Guide**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession is the new entry point to interact with Spark functionalities. Befor Spark 2.0.0, we needed to create different SparkContext for these functionalities. When starting Spark on an interactive mode, a SparkSession is automatically created. In this notebook, we need to do that manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T16:58:05.368543Z",
     "start_time": "2020-07-20T16:57:54.942144Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# local means that driver and executers run on your individual\n",
    "# computer instead of a cluster. \n",
    "spark = SparkSession.builder.master(\"local\").appName(\"practice\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T16:58:05.374136Z",
     "start_time": "2020-07-20T16:58:05.370536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001EB865C34E0>\n"
     ]
    }
   ],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T16:29:45.350381Z",
     "start_time": "2020-05-14T16:29:45.334624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://BRITOD.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a7633cacf8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataFrame**  \n",
    "Table of data with rows and columns. Similar to a DataFrame, but distributed into partitions (groups of rows on different clusters). Creating a df from a range of numbers is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T16:58:29.811102Z",
     "start_time": "2020-07-20T16:58:28.133021Z"
    }
   },
   "outputs": [],
   "source": [
    "my_range = spark.range(1000).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T18:52:58.092670Z",
     "start_time": "2020-05-12T18:52:57.971993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# narrow transformation (fast)\n",
    "divis_by2 = my_range.where(\"number % 2 = 0\")\n",
    "# wide transformation + collect (slow since it uses shuffling)\n",
    "divis_by2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from CSV file (reading data in Spark is also a transformation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T18:53:30.405979Z",
     "start_time": "2020-05-12T18:53:27.956033Z"
    }
   },
   "outputs": [],
   "source": [
    "# using schema inference \n",
    "r_packages = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\") \\\n",
    "                  .csv(\"./data/r_packages_log.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing an execution plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T18:53:55.914852Z",
     "start_time": "2020-05-12T18:53:55.867978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [size#30 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(size#30 ASC NULLS FIRST, 200)\n",
      "   +- *(1) FileScan csv [date#28,time#29,size#30,r_version#31,r_arch#32,r_os#33,package#34,version#35,country#36,ip_id#37] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/USUARIO/Dropbox/ds_code/git-projects/spark-toolbox/data/r_packag..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<date:timestamp,time:string,size:int,r_version:string,r_arch:string,r_os:string,package:str...\n"
     ]
    }
   ],
   "source": [
    "r_packages.sort(\"size\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T18:54:05.210433Z",
     "start_time": "2020-05-12T18:54:02.803915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date=datetime.datetime(2015, 12, 12, 0, 0), time='19:06:56', size=504, r_version='NA', r_arch='NA', r_os='NA', package='httpRequest', version='0.0.5', country='CN', ip_id=1133),\n",
       " Row(date=datetime.datetime(2015, 12, 12, 0, 0), time='20:33:02', size=504, r_version='NA', r_arch='NA', r_os='NA', package='granova', version='1.4', country='CN', ip_id=5331)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 200 is the standard number of partitions\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "r_packages.sort(\"size\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "SQL queries and DataFrame code both compile to Spark code, so there is no difference in performance. Before using SQL commnads, we need to create a table or a view. We can do this from a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T18:56:30.734138Z",
     "start_time": "2020-05-12T18:56:30.709205Z"
    }
   },
   "outputs": [],
   "source": [
    "r_packages.createOrReplaceTempView(\"r_packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can execute SQL statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T18:56:35.507090Z",
     "start_time": "2020-05-12T18:56:35.427294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[size#30], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(size#30, 200)\n",
      "   +- *(1) HashAggregate(keys=[size#30], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [size#30] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/USUARIO/Dropbox/ds_code/git-projects/spark-toolbox/data/r_packag..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<size:int>\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    "SELECT size, count(1) \n",
    "FROM r_packages\n",
    "GROUP BY size\n",
    "\"\"\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import several transformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T18:56:56.262615Z",
     "start_time": "2020-05-12T18:56:55.714078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(size)=68736865)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "r_packages.select(max(\"size\")).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex query using SQL (top 5 average package sizes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T18:57:07.677513Z",
     "start_time": "2020-05-12T18:57:03.714107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|country|          avg_size|\n",
      "+-------+------------------+\n",
      "|     IQ|         4306891.6|\n",
      "|     IS| 3690250.714285714|\n",
      "|     SV|         3557710.1|\n",
      "|     KW|2896512.4444444445|\n",
      "|     GE|        2672563.25|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT country, AVG(size) as avg_size\n",
    "FROM r_packages\n",
    "GROUP BY country\n",
    "ORDER BY avg_size DESC\n",
    "LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using Spark functional paradigm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T18:57:25.087017Z",
     "start_time": "2020-05-12T18:57:22.889317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|country|          avg_size|\n",
      "+-------+------------------+\n",
      "|     IQ|         4306891.6|\n",
      "|     IS| 3690250.714285714|\n",
      "|     SV|         3557710.1|\n",
      "|     KW|2896512.4444444445|\n",
      "|     GE|        2672563.25|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "r_packages.groupBy(\"country\").avg(\"size\").withColumnRenamed(\"avg(size)\", \"avg_size\") \\\n",
    "          .sort(desc(\"avg_size\")).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T18:57:30.066230Z",
     "start_time": "2020-05-12T18:57:30.028304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[avg_size#187 DESC NULLS LAST], output=[country#36,avg_size#187])\n",
      "+- *(2) HashAggregate(keys=[country#36], functions=[avg(cast(size#30 as bigint))])\n",
      "   +- Exchange hashpartitioning(country#36, 200)\n",
      "      +- *(1) HashAggregate(keys=[country#36], functions=[partial_avg(cast(size#30 as bigint))])\n",
      "         +- *(1) FileScan csv [size#30,country#36] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/USUARIO/Dropbox/ds_code/git-projects/spark-toolbox/data/r_packag..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<size:int,country:string>\n"
     ]
    }
   ],
   "source": [
    "r_packages.groupBy(\"country\").avg(\"size\").withColumnRenamed(\"avg(size)\", \"avg_size\") \\\n",
    "          .sort(desc(\"avg_size\")).limit(5).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T18:59:48.742558Z",
     "start_time": "2020-05-12T18:59:48.738536Z"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'by-day' folder contains one csv file for every day in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:00:28.799651Z",
     "start_time": "2020-05-12T19:00:28.796658Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:00:57.690877Z",
     "start_time": "2020-05-12T19:00:46.341049Z"
    }
   },
   "outputs": [],
   "source": [
    "# Infering schema from several file\n",
    "static_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "                 .load(DATA_PATH)\n",
    "# Create view/table\n",
    "static_df.createOrReplaceTempView(\"retail_data\")\n",
    "static_schema = static_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of query with window function (windows over time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T21:44:55.147933Z",
     "start_time": "2020-04-21T21:44:51.483011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   14075.0|[2011-12-04 19:00...|316.78000000000003|\n",
      "|   18180.0|[2011-12-04 19:00...|            310.73|\n",
      "|   15358.0|[2011-12-04 19:00...| 830.0600000000003|\n",
      "|   15392.0|[2011-12-04 19:00...|304.40999999999997|\n",
      "|   15290.0|[2011-12-04 19:00...|263.02000000000004|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, column, desc, col\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "static_df.selectExpr(\"CustomerId\", \"(UnitPrice * Quantity) as total_cost\", \"InvoiceDate\") \\\n",
    "         .groupBy(\"CustomerId\", window(col(\"InvoiceDate\"), \"1 day\")).sum(\"total_cost\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using readStream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T22:04:15.800170Z",
     "start_time": "2020-04-21T22:04:14.749718Z"
    }
   },
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream.schema(static_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T22:04:58.841937Z",
     "start_time": "2020-04-21T22:04:58.836946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_df.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T22:09:54.433922Z",
     "start_time": "2020-04-21T22:09:54.391031Z"
    }
   },
   "outputs": [],
   "source": [
    "purchase_by_costumer_per_hour = streaming_df\\\n",
    "    .selectExpr(\n",
    "      \"CustomerId\",\n",
    "      \"(UnitPrice * Quantity) as total_cost\",\n",
    "      \"InvoiceDate\")\\\n",
    "    .groupby(\n",
    "      col(\"CustomerId\"), window(\"InvoiceDate\", \"1 day\"))\\\n",
    "    .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process the stream data and write it on a in memory table. The table is update after each trigger (defined above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T22:13:25.676118Z",
     "start_time": "2020-04-21T22:13:24.247734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x214209bc048>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchase_by_costumer_per_hour.writeStream\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"customer_purchases\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T22:30:00.233680Z",
     "start_time": "2020-04-21T22:29:56.531267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   15290.0|[2011-02-21 19:00...|             -1.65|\n",
      "|   14911.0|[2011-03-10 19:00...|               0.0|\n",
      "|   15208.0|[2010-12-20 19:00...|              65.4|\n",
      "|   15694.0|[2011-03-15 20:00...|            584.76|\n",
      "|   12921.0|[2011-03-29 20:00...|-87.30000000000001|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM customer_purchases\n",
    "ORDER BY 'sum(total_cost)' DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data (same data used on the streaming example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T22:34:38.153606Z",
     "start_time": "2020-04-21T22:34:25.402833Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv\"\n",
    "# Infering schema from several file\n",
    "static_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "                 .load(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T22:33:59.736541Z",
     "start_time": "2020-04-21T22:33:59.731563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing our data (withColumn is used to add a new column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T22:44:09.879651Z",
     "start_time": "2020-04-21T22:44:09.847770Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "prepared_df = static_df\\\n",
    ".na.fill(0)\\\n",
    ".withColumn(\"day_of_week\", date_format(\"InvoiceDate\", \"EEEE\"))\\\n",
    ".coalesce(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T22:46:43.649492Z",
     "start_time": "2020-04-21T22:46:43.632536Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = prepared_df.where(\"InvoiceDate < '2011-07-01'\")\n",
    "train_df = prepared_df.where(\"InvoiceDate >= '2011-07-01'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting encoder for day of the week column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T23:00:54.451070Z",
     "start_time": "2020-04-21T23:00:54.438074Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple Label Encoder (changes to integers)\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer()\\\n",
    ".setInputCol(\"day_of_week\")\\\n",
    ".setOutputCol(\"day_of_week_index\")\n",
    "\n",
    "# One Hot Encoder (works on top of the previous transformation)\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder()\\\n",
    ".setInputCol(\"day_of_week_index\")\\\n",
    ".setOutputCol(\"day_of_week_encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to assemble the columns we will use into a vector (ML algorithms in Spark take as input a Vector type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T23:00:56.286093Z",
     "start_time": "2020-04-21T23:00:56.278113Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vector_assembler = VectorAssembler()\\\n",
    ".setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
    ".setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T23:00:59.274291Z",
     "start_time": "2020-04-21T23:00:59.270301Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformation_pipeline = Pipeline()\\\n",
    ".setStages([indexer, encoder, vector_assembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T23:01:06.646405Z",
     "start_time": "2020-04-21T23:01:00.478099Z"
    }
   },
   "outputs": [],
   "source": [
    "fitted_pipeline = transformation_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T23:01:08.446135Z",
     "start_time": "2020-04-21T23:01:08.203782Z"
    }
   },
   "outputs": [],
   "source": [
    "transformed_training = fitted_pipeline.transform(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training KMeans with caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T23:05:03.985861Z",
     "start_time": "2020-04-21T23:05:03.972896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string, day_of_week: string, day_of_week_index: double, day_of_week_encoded: vector, features: vector]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_training.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T23:05:17.738008Z",
     "start_time": "2020-04-21T23:05:17.734020Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T23:05:55.166385Z",
     "start_time": "2020-04-21T23:05:55.142450Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans()\\\n",
    ".setK(20)\\\n",
    ".setSeed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naming patter in Spark: kmeans for untrained model and kmeans_model for trained version. Let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T23:07:39.426863Z",
     "start_time": "2020-04-21T23:07:05.150827Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans_model = kmeans.fit(transformed_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T23:08:13.007409Z",
     "start_time": "2020-04-21T23:08:12.222506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98518684.09312333"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Horrible in this case -> we need to scale the date for KNN\n",
    "kmeans_model.computeCost(transformed_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured API (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review**: The user specifies multiple *transformations* that are build on an acyclic graph of instructions. An *action* begins the process of executing the graph of instructions. Spark breaks down into stages and executes across the cluster. The logical structures for transformations and actions are DataFrames and Datasets.   \n",
    "**Note**: Tables and views are basically the same thing as DataFrames. We just use SQL against them instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring column type in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T20:15:13.912454Z",
     "start_time": "2020-05-12T20:15:13.909463Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "b = ByteType()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check pg.59 of the book to find equivalence between Python data types and Spark's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Structured Operations (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames consist of records of type Row, and a number of columns (logical constructions that represent value computed on a per-record basis by means of an expression). Schemas define the name and type of data in each column. Partition is the layout of the DF across the cluster. Partitioning schema defines how that is allocated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:45:31.500362Z",
     "start_time": "2020-05-13T20:45:27.828428Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json\"\n",
    "df = spark.read.format(\"json\").load(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:45:37.118619Z",
     "start_time": "2020-05-13T20:45:37.112629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A schema is composed of StructType(list or tuple) of StructFields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:45:38.334593Z",
     "start_time": "2020-05-13T20:45:38.329609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: define your schemas manually in production, instead of using schema-on-read. This is more robust and efficient. We have to use Spark data types for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:45:45.279814Z",
     "start_time": "2020-05-13T20:45:45.264827Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "my_schema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"json\").schema(my_schema).load(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:45:45.784713Z",
     "start_time": "2020-05-13T20:45:45.779697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering a temporary view so we can use SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T21:02:03.923604Z",
     "start_time": "2020-05-13T21:02:03.774404Z"
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**select and selectExpr**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T21:04:14.569127Z",
     "start_time": "2020-05-13T21:04:14.473549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T21:07:00.835495Z",
     "start_time": "2020-05-13T21:07:00.755347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "|  destination|ORIGIN_COUNTRY_NAME|\n",
      "+-------------+-------------------+\n",
      "|United States|            Romania|\n",
      "|United States|            Croatia|\n",
      "+-------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"DEST_COUNTRY_NAME AS destination\", \"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use any valid non-aggregating SQL statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T21:08:55.943945Z",
     "start_time": "2020-05-13T21:08:55.879214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"*\", \n",
    "              \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n",
    "             .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or aggregations over the entire DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T21:24:35.458749Z",
     "start_time": "2020-05-13T21:24:32.219871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Literals** values coming from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T21:30:56.942837Z",
     "start_time": "2020-05-13T21:30:56.882931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15| 10|\n",
      "|    United States|            Croatia|    1| 10|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, expr\n",
    "\n",
    "a=10\n",
    "df.select(expr(\"*\"), lit(a).alias(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove** column (you can also use a SELECT statement):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T21:36:55.303967Z",
     "start_time": "2020-05-13T21:36:55.243710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|DEST_COUNTRY_NAME|count|\n",
      "+-----------------+-----+\n",
      "|    United States|   15|\n",
      "|    United States|    1|\n",
      "+-----------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The transformation is not inplace=True\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cast** (changing column type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T21:38:29.877102Z",
     "start_time": "2020-05-13T21:38:29.818283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|count2|\n",
      "+-----------------+-------------------+-----+------+\n",
      "|    United States|            Romania|   15|    15|\n",
      "|    United States|            Croatia|    1|     1|\n",
      "+-----------------+-------------------+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumn adds or replaces and existing column\n",
    "df.withColumn(\"count2\", col(\"count\").cast(\"string\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filter** rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T00:48:07.289595Z",
     "start_time": "2020-05-14T00:48:07.221779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"count<2\").show(2)\n",
    "#df.filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T00:49:37.643856Z",
     "start_time": "2020-05-14T00:49:37.570062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"DEST_COUNTRY_NAME\")=='United States').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T00:50:25.033525Z",
     "start_time": "2020-05-14T00:50:24.943737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"count<2\").where(col(\"DEST_COUNTRY_NAME\")=='United States').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**unique rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T00:54:41.064717Z",
     "start_time": "2020-05-14T00:54:38.254140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T00:54:51.667214Z",
     "start_time": "2020-05-14T00:54:49.640588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sampling and random split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T00:56:14.800509Z",
     "start_time": "2020-05-14T00:56:14.735681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(withReplacement=False, fraction=0.5, seed=5).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T00:58:03.690008Z",
     "start_time": "2020-05-14T00:58:03.623159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proportions will be normalized if not adding up to 1\n",
    "df.randomSplit([0.25, 0.75], seed=5)[0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T00:57:10.959155Z",
     "start_time": "2020-05-14T00:57:10.880335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.randomSplit([0.25, 0.75], seed=5)[1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**concatenating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:23:56.119182Z",
     "start_time": "2020-05-14T01:23:56.096245Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating a new DF\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = df.schema\n",
    "newRows = [\n",
    "    Row(\"New Country\", \"Other Country\", 5),\n",
    "    Row(\"New Country 2\", \"Other Country 2\", 5)\n",
    "]\n",
    "newDF = spark.createDataFrame(newRows, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:23:58.214575Z",
     "start_time": "2020-05-14T01:23:57.485295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|      New Country|      Other Country|    5|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.union(newDF).where(col(\"ORIGIN_COUNTRY_NAME\") == \"Other Country\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sorting**   \n",
    "There are several ways to do it. asc() is used to make the command explict, but it is the standard behavior. Use asc_nulls_first, desc_nulls_last, etc. to specify where you want the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:32:40.971021Z",
     "start_time": "2020-05-14T01:32:40.801442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "df.sort(\"count\").show(2)\n",
    "df.orderBy(col(\"count\").desc()).show(2)\n",
    "df.orderBy(expr(\"count desc\")).show(2)\n",
    "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**repartitioning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:40:51.522222Z",
     "start_time": "2020-05-14T01:40:51.506269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get current number of partitions\n",
    "df.rdd.getNumPartitions()\n",
    "# repartition by number \n",
    "df.repartition(4)\n",
    "# repartition by column (useful if we filter by some column frequently)\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))\n",
    "# coalesce\n",
    "df.coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Types of Data (6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T16:32:18.856266Z",
     "start_time": "2020-05-14T16:32:18.469502Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../Spark-The-Definitive-Guide/data/retail-data/by-day/2010-12-01.csv\"\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "          .option(\"inferSchema\", \"true\")\\\n",
    "          .load(DATA_PATH)\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T16:32:52.463631Z",
     "start_time": "2020-05-14T16:32:52.166162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3108"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**converting types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T23:31:31.383481Z",
     "start_time": "2020-05-14T23:31:31.271781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  5|  5|\n",
      "+---+---+\n",
      "|  5|  5|\n",
      "|  5|  5|\n",
      "|  5|  5|\n",
      "|  5|  5|\n",
      "|  5|  5|\n",
      "+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.select(lit(5), lit(\"5\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**booleans and filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T16:52:17.768683Z",
     "start_time": "2020-05-14T16:52:17.656571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr, col\n",
    "\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**working with numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:00:21.273215Z",
     "start_time": "2020-05-14T17:00:21.202439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|            Result|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, pow\n",
    "\n",
    "expression = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerId\") ,expression.alias(\"Result\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:06:28.981411Z",
     "start_time": "2020-05-14T17:06:28.785651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "\n",
    "df.stat.corr(\"Quantity\", \"UnitPrice\")\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe like in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:07:04.397522Z",
     "start_time": "2020-05-14T17:07:04.161465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:09:09.392707Z",
     "start_time": "2020-05-14T17:09:09.386717Z"
    }
   },
   "source": [
    "**strings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Expressions: regexp_extract and regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:20:15.148485Z",
     "start_time": "2020-05-14T17:20:15.091758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       Changed Sting|\n",
      "+--------------------+\n",
      "|AAAA HANGING HEAR...|\n",
      "|  AAAA METAL LANTERN|\n",
      "|CREAM CUPID HEART...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract, regexp_replace, translate\n",
    "\n",
    "regex_string = 'BLACK|WHITE|RED|GREEN|BLUE'\n",
    "df.select(regexp_replace(col(\"Description\"), regex_string, \"AAAA\").alias(\"Changed Sting\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate is easier for character substitution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:21:08.209525Z",
     "start_time": "2020-05-14T17:21:08.115916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         Description|      Changed Column|\n",
      "+--------------------+--------------------+\n",
      "|WHITE HANGING HEA...|WHI73 HANGING H3A...|\n",
      "| WHITE METAL LANTERN| WHI73 M37A1 1AN73RN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"Description\"), translate(col(\"Description\"), 'LEET', '1337').alias('Changed Column')).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function generating columns with booleans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:27:47.227844Z",
     "start_time": "2020-05-14T17:27:47.208318Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import locate\n",
    "\n",
    "simple_colors = [\"black\", \"red\", \"white\"]\n",
    "def color_locator(column, color_string):\n",
    "    return locate(color_string.upper(), column)\\\n",
    "           .cast(\"boolean\").alias(\"is_\"+color_string)\n",
    "\n",
    "selected_columns = [color_locator(df.Description, c) for c in simple_colors]\n",
    "# trick to add other columns when using select\n",
    "selected_columns.append(expr(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:28:41.733467Z",
     "start_time": "2020-05-14T17:28:41.672637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(*selected_columns).where(expr(\"is_white or is_red\")).select(\"Description\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T23:27:44.046001Z",
     "start_time": "2020-05-14T23:27:44.023095Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "dateFormat = 'yyyy-dd-MM'\n",
    "cleanDateDF = spark.range(1).select(\n",
    "    to_date(lit('2017-12-11'), dateFormat).alias('date'),\n",
    "    to_date(lit('2017-20-11'), dateFormat).alias('date2')\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T23:27:45.086491Z",
     "start_time": "2020-05-14T23:27:45.035663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-11-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**null**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T23:34:15.994838Z",
     "start_time": "2020-05-14T23:34:15.935996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
      "|ifnull(NULL, 'return_value')|nullif('value', 'value')|nvl(NULL, 'return_value')|nvl2('not_null', 'return_value', 'else_value')|\n",
      "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
      "|                return_value|                    null|             return_value|                                  return_value|\n",
      "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    ifnull(null, 'return_value'),\n",
    "    nullif('value', 'value'),\n",
    "    nvl(null, 'return_value'),\n",
    "    nvl2('not_null', 'return_value', 'else_value')\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**structs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T23:40:20.009518Z",
     "start_time": "2020-05-14T23:40:19.990539Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"compleDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T23:41:42.688208Z",
     "start_time": "2020-05-14T23:41:42.550577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| complex.Description|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+\n",
      "|         Description|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+---------+\n",
      "|         Description|InvoiceNo|\n",
      "+--------------------+---------+\n",
      "|WHITE HANGING HEA...|   536365|\n",
      "| WHITE METAL LANTERN|   536365|\n",
      "+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.select(col(\"complex\").getField(\"Description\")).show(2)\n",
    "complexDF.select(\"complex.Description\").show(2)\n",
    "complexDF.select(\"complex.*\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**arrays**: very useful when splitting information in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T23:54:01.157006Z",
     "start_time": "2020-05-14T23:54:01.002253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "|       CREAM|\n",
      "|     KNITTED|\n",
      "+------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+----+\n",
      "|size|\n",
      "+----+\n",
      "|   5|\n",
      "|   3|\n",
      "|   5|\n",
      "|   6|\n",
      "+----+\n",
      "only showing top 4 rows\n",
      "\n",
      "+------+\n",
      "|WHITE?|\n",
      "+------+\n",
      "|  true|\n",
      "|  true|\n",
      "| false|\n",
      "| false|\n",
      "+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, size, array_contains\n",
    "\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\")).selectExpr(\"array_col[0]\").show(4)\n",
    "\n",
    "df.select(size(split(col(\"Description\"), \" \")).alias(\"size\")).show(4)\n",
    "\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\").alias(\"WHITE?\")).show(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**explode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T23:43:10.159405Z",
     "start_time": "2020-05-15T23:43:10.034740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+\n",
      "|         Description|InvoiceNo|exploded|\n",
      "+--------------------+---------+--------+\n",
      "|WHITE HANGING HEA...|   536365|   WHITE|\n",
      "|WHITE HANGING HEA...|   536365| HANGING|\n",
      "|WHITE HANGING HEA...|   536365|   HEART|\n",
      "+--------------------+---------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n",
    "  .withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    "  .select(\"Description\", \"InvoiceNo\", \"exploded\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**maps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T20:28:53.828248Z",
     "start_time": "2020-05-16T20:28:53.754172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|map(Description, UnitPrice)                 |\n",
      "+--------------------------------------------+\n",
      "|[WHITE HANGING HEART T-LIGHT HOLDER -> 2.55]|\n",
      "|[WHITE METAL LANTERN -> 3.39]               |\n",
      "+--------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "df.select(create_map(col(\"Description\"), col(\"UnitPrice\"))).show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T20:36:43.929211Z",
     "start_time": "2020-05-16T20:36:43.916214Z"
    }
   },
   "outputs": [],
   "source": [
    "jsonDF = spark.range(1).selectExpr(\"\"\"\n",
    "'{\"myJSONKey\" : {\"myJSONValue\" : [1,2,3]}}' as jsonString\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T20:36:44.686956Z",
     "start_time": "2020-05-16T20:36:44.664019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          jsonString|\n",
      "+--------------------+\n",
      "|{\"myJSONKey\" : {\"...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single level or multiple levels access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T20:45:48.939021Z",
     "start_time": "2020-05-16T20:45:48.888998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|column|                  c0|\n",
      "+------+--------------------+\n",
      "|     2|{\"myJSONValue\":[1...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "\n",
    "jsonDF.select(\n",
    "    # multiple levels access\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias(\"column\"),\n",
    "    # One level of nesting\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing structs to json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T20:50:45.587628Z",
     "start_time": "2020-05-16T20:50:45.533083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|JSON                                                                     |\n",
      "+-------------------------------------------------------------------------+\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}|\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}               |\n",
      "+-------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(struct('InvoiceNo', 'Description').alias(\"myStruct\"))\\\n",
    "  .selectExpr(\"to_json(myStruct) as JSON\").show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Functions (UDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to write custom functions using Scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:03:29.648111Z",
     "start_time": "2020-05-16T21:03:28.989571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "|         27|\n",
      "|         64|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "\n",
    "# we need to register the custom function \n",
    "from pyspark.sql.functions import udf, col\n",
    "power3udf = udf(power3)\n",
    "# applying the custom function\n",
    "udfExampleDF.select(power3udf(col(\"num\"))).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:33:33.116695Z",
     "start_time": "2020-05-25T15:33:23.859779Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# local meansthat driver and executers run on your individual\n",
    "# computer instead of a cluster. \n",
    "spark = SparkSession.builder.master(\"local\").appName(\"practice\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:33:33.122642Z",
     "start_time": "2020-05-25T15:33:33.118620Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../Spark-The-Definitive-Guide/data/retail-data/all/*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:33:38.285186Z",
     "start_time": "2020-05-25T15:33:33.124633Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(DATA_PATH)\\\n",
    "    .coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:33:38.501295Z",
     "start_time": "2020-05-25T15:33:38.287134Z"
    }
   },
   "outputs": [],
   "source": [
    "# for rapid access\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the chapter for more examples on: count, countDistinct, approx_count_distinct, first/last, min/max, sum, sumDistinct, avg, variance (sample and population - be careful), skewness, kurtosis, covariance/correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sets and lists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T20:17:44.982198Z",
     "start_time": "2020-05-24T20:17:44.094613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T20:20:36.791233Z",
     "start_time": "2020-05-24T20:20:36.255036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "+---------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using agg and expressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T20:21:40.018827Z",
     "start_time": "2020-05-24T20:21:39.476389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---------------+\n",
      "|InvoiceNo|quant|count(Quantity)|\n",
      "+---------+-----+---------------+\n",
      "|   536596|    6|              6|\n",
      "|   536938|   14|             14|\n",
      "|   537252|    1|              1|\n",
      "|   537691|   20|             20|\n",
      "|   538041|    1|              1|\n",
      "+---------+-----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    count(\"Quantity\").alias(\"quant\"),\n",
    "    expr(\"count(Quantity)\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T20:22:42.503361Z",
     "start_time": "2020-05-24T20:22:41.005228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+---------------+\n",
      "|InvoiceNo|                SD|count(Quantity)|\n",
      "+---------+------------------+---------------+\n",
      "|   536596|1.1180339887498947|              6|\n",
      "|   536938|20.698023172885524|             14|\n",
      "|   537252|               0.0|              1|\n",
      "|   537691| 5.597097462078001|             20|\n",
      "|   538041|               0.0|              1|\n",
      "+---------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    expr(\"stddev_pop(Quantity)\").alias(\"SD\"),\n",
    "    expr(\"count(Quantity)\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:34:20.174624Z",
     "start_time": "2020-05-25T15:34:20.107187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|   InvoiceDate|\n",
      "+--------------+\n",
      "|12/1/2010 8:26|\n",
      "+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"InvoiceDate\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:34:40.914578Z",
     "start_time": "2020-05-25T15:34:40.840406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2010-12-01|\n",
      "+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.select(\"date\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:45:18.691707Z",
     "start_time": "2020-05-25T15:45:18.658763Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, desc, max, dense_rank, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "# we want the rank for customer for each day\n",
    "# we need this for rank\n",
    "windowSpec = Window.partitionBy(\"CustomerId\", \"date\")\\\n",
    "                   .orderBy(desc(\"Quantity\"))\\\n",
    "                   .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# we are creating unresolved columns ahead of time below\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)\n",
    "\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:46:35.169013Z",
     "start_time": "2020-05-25T15:46:33.217629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|              74215|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|              74215|\n",
      "|     12347|2010-12-07|      36|           1|                1|                 36|\n",
      "|     12347|2010-12-07|      30|           2|                2|                 36|\n",
      "|     12347|2010-12-07|      24|           3|                3|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.where(\"CustomerId IS NOT NULL\")\\\n",
    "          .orderBy(\"CustomerId\")\\\n",
    "          .select(col(\"CustomerId\"), \n",
    "                  col(\"date\"), col(\"Quantity\"),\n",
    "                  purchaseRank.alias(\"quantityRank\"),\n",
    "                  purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "                  maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")\n",
    "                  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T16:20:20.955059Z",
     "start_time": "2020-05-25T16:20:20.937107Z"
    }
   },
   "outputs": [],
   "source": [
    "dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T16:21:43.873120Z",
     "start_time": "2020-05-25T16:21:41.632272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|customerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "|     18287|   85039A|           96|\n",
      "|     18287|    84920|            4|\n",
      "|     18287|    84584|            6|\n",
      "|     18287|   84507C|            6|\n",
      "|     18287|   72351B|           24|\n",
      "|     18287|   72351A|           24|\n",
      "|     18287|   72349B|           60|\n",
      "+----------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\n",
    "GROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode), ())\n",
    "ORDER BY CustomerId DESC, stockCode DESC \n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:playground]",
   "language": "python",
   "name": "conda-env-playground-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.078px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
