{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing for DS in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CI**: run all unit tests when code is pushed, preventing \"bad code\" going to production.\n",
    "\n",
    "**Unit**: any small, independent piece of code.\n",
    "\n",
    "**Integration test**: tests if multiple units run well together, and not independently.\n",
    "\n",
    "**End-to-end test**: whole software at once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit testing basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic test files**  \n",
    "**test_** indicates unit test inside a file (like test_row_to_list.py, for example). These files are also called **test modules**. Example of test file test_row_to_list.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import row_to_list\n",
    "\n",
    "# test_ indicates a unit test, not a regular function\n",
    "def test_for_clean_row():\n",
    "    # assert boolean_expression\n",
    "    assert row_to_list(value) == expected_value\n",
    "\n",
    "# case when the function needs to return None\n",
    "# make sure to use is when using None\n",
    "def test_for_missing_value():\n",
    "    assert row_to_list[\"\\t29,0\\t\"] is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pytest package\n",
    "import pytest\n",
    "\n",
    "# Import the function convert_to_int()\n",
    "from preprocessing_helpers import convert_to_int\n",
    "\n",
    "# Complete the unit test name by adding a prefix\n",
    "def test_on_string_with_one_comma():\n",
    "  # Complete the assert statement\n",
    "  assert convert_to_int(\"2,081\") == 2081"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytest test_row_to_list.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "- . represents pass and F failure\n",
    "- A test could fail with an AssertionError (the function has a bug and needs to be fixed).\n",
    "- For other types of exception (like NameError), there is something wrong with the test (before the assert is verified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate unit testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional second argument (for assertion error):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_for_missing_value_with_message():\n",
    "    \n",
    "    actual = row_to_list(\"\\t293,410\\t\")\n",
    "    \n",
    "    expected = None\n",
    "    \n",
    "    message = (\"row_to_list('\\t293,410\\t') \"\n",
    "               \"returned {0} instead\"\n",
    "               \"of {1}\".format(actual, expected)\n",
    "              )\n",
    "    \n",
    "    assert row_to_list[\"\\t29,0\\t\"] is None, message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When **comparing floats**, use pytest.approx():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T01:15:10.484260Z",
     "start_time": "2020-08-11T01:15:10.481259Z"
    }
   },
   "outputs": [],
   "source": [
    "assert 0.1 + 0.1 + 0.1 == pytest.approx(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also works with NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T01:16:14.893576Z",
     "start_time": "2020-08-11T01:16:14.889577Z"
    }
   },
   "outputs": [],
   "source": [
    "assert np.array([0.1 + 0.1]) == pytest.approx(np.array([0.2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple assert statements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T21:30:17.120775Z",
     "start_time": "2020-08-12T21:30:16.938591Z"
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "\n",
    "def test_on_string_with_one_comma():\n",
    "    return_value = convert_to_int(\"2,081\")\n",
    "    \n",
    "    assert isinstance(return_value, int)\n",
    "    assert return_value ==2081"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General template for a context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with context_manager:\n",
    "    # Does something when entering context\n",
    "    print(\"This is part of the context\")\n",
    "    # Does something when leaving the context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pytest .raises(arg) to **test for exceptions**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(ValueError):\n",
    "    # Does nothing\n",
    "    print(\"Part of the context\")\n",
    "    # If context raised ValueError, silence it\n",
    "    # If context did not raise ValueError, raise an execption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T21:33:49.539737Z",
     "start_time": "2020-08-12T21:33:49.536737Z"
    }
   },
   "outputs": [],
   "source": [
    "# The test passes when the right exception is raised\n",
    "with pytest.raises(ValueError):\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T21:33:43.801054Z",
     "start_time": "2020-08-12T21:33:43.786061Z"
    }
   },
   "outputs": [
    {
     "ename": "Failed",
     "evalue": "DID NOT RAISE <class 'ValueError'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailed\u001b[0m                                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0510db8df69e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mpytest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraises\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dl\\lib\\site-packages\\_pytest\\python_api.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, *tp)\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mfail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexcinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[0msuppress_exception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexcinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpected_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dl\\lib\\site-packages\\_pytest\\outcomes.py\u001b[0m in \u001b[0;36mfail\u001b[1;34m(msg, pytrace)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \"\"\"\n\u001b[0;32m    118\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mFailed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpytrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpytrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailed\u001b[0m: DID NOT RAISE <class 'ValueError'>"
     ]
    }
   ],
   "source": [
    "# The test fails if not exception is raised\n",
    "with pytest.raises(ValueError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we need to check the exception message? Capture the exception and check the message on it after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T21:45:26.815799Z",
     "start_time": "2020-08-12T21:45:26.811802Z"
    }
   },
   "outputs": [],
   "source": [
    "with pytest.raises(ValueError) as exception:\n",
    "    raise ValueError('a')\n",
    "assert exception.match(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the ValueError passes, but the exception message raises an assertion error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T21:45:34.155546Z",
     "start_time": "2020-08-12T21:45:34.143550Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Pattern 'b' not found in 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-99fec6552922>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mpytest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraises\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dl\\lib\\site-packages\\_pytest\\_code\\code.py\u001b[0m in \u001b[0;36mmatch\u001b[1;34m(self, regexp)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m             \u001b[1;32massert\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Pattern '{!s}' not found in '{!s}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Pattern 'b' not found in 'a'"
     ]
    }
   ],
   "source": [
    "with pytest.raises(ValueError) as exception:\n",
    "    raise ValueError('a')\n",
    "assert exception.match(\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **well tested function** has tests for these argument types:\n",
    "- Bad arguments: arguments that raise an exception when passed;\n",
    "- Special arguments: boundary values (neighbors of the acceptable values) and arguments with special logic;\n",
    "- Normal arguments: not any of the above.\n",
    "\n",
    "Recommended: one for each special case and at least 2 for normal  arguments. \n",
    "\n",
    "**Tip**: using mapping from arguments to tuples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Driven Development** (TDD): writing unit tests before implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Organization and Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test folders mirrors the application folder. Python module and test module correspondence is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test class** (PyTest): container for a single unit's tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T01:39:09.404223Z",
     "start_time": "2020-08-13T01:39:09.400222Z"
    }
   },
   "outputs": [],
   "source": [
    "class TestFunctionName(object): # Always put the argument object\n",
    "    def test_case1(self):\n",
    "        \n",
    "    def test_case2(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Models, Plots and Much More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of workflow: setup, assert, and teardown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_raw_data():\n",
    "    # Setup: create the raw data file\n",
    "    preprocess(raw_data_file_path,\n",
    "               clean_data_file_path)\n",
    "    \n",
    "    with open(clean_data_file_path) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    first_line = lines[0]\n",
    "    assert first_line == '1801\\t20'\n",
    "    \n",
    "    second_line = lines[1]\n",
    "    assert second_line == '2002\\t333'\n",
    "    \n",
    "    # Teardown: remove raw and clean data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTest, we use **fixture** for the setup and teardown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def raw_and_clean_data_file():\n",
    "    # Do setup here\n",
    "    \n",
    "    yield data\n",
    "    \n",
    "    # Do teardown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_something(my_fixture):\n",
    "    ...\n",
    "    data = my_fixture\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def raw_and_clean_data_file():\n",
    "    raw_data_file_path = \"raw.txt\"\n",
    "    clean_data_file_path = \"clean.txt\"\n",
    "    \n",
    "    with open(raw_data_file_path, \"w\") as f:\n",
    "        f.write(\"asasasas\"\n",
    "                \"asfrgehr\")\n",
    "    \n",
    "    yield raw_data_file_path, clean_data_file_path\n",
    "    \n",
    "    os.remove(raw_data_file_path)\n",
    "    os.remove(clean_data_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytest\n",
    "\n",
    "def test_on_raw_data(raw_and_clean_data_file):\n",
    "    \n",
    "    raw_path, clean_path = raw_and_clean_data_file\n",
    "    preprocess(raw_path, clean_path)\n",
    "    \n",
    "    with open(clean_data_file_path) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    first_line = lines[0]\n",
    "    assert first_line == '1801\\t20'\n",
    "    \n",
    "    second_line = lines[1]\n",
    "    assert second_line == '2002\\t333'\n",
    "    \n",
    "    # Teardown: remove raw and clean data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTest provides built-in fixtures, like **tmpdir**. It creates a temporary directory during setup and deletes it during teardown. We can do **fixture chaining** using tmpdir and our own fixture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup of tmpdir() -> setup of raw_and_clean_data_file\n",
    "# -> test -> teardown of raw_and_clean_data_file() \n",
    "# -> teardown of tmpdir()\n",
    "\n",
    "@pytest.fixture\n",
    "def raw_and_clean_data_file(tmpdir):\n",
    "    \n",
    "    raw_data_file_path = tmpdir.join(\"raw.txt\")\n",
    "    clean_data_file_path = tmpdir.join(\"clean.txt\")\n",
    "    \n",
    "    with open(raw_data_file_path, \"w\") as f:\n",
    "        f.write(\"asasasas\"\n",
    "                \"asfrgehr\")\n",
    "    \n",
    "    yield raw_data_file_path, clean_data_file_path\n",
    "    \n",
    "    # Now we can ommit the teardown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mocking**  \n",
    "- Test results should not depend on dependencies, but on the behavior of the function being tested.\n",
    "- Mocking is testing functions independently of dependencies.\n",
    "- We need two packages: **pytest-mock** and unittest.mock\n",
    "\n",
    "\n",
    "**MagicMock** and **mocker.patch()**  \n",
    "- We want to replace potentially buggy dependencies with unittest.mock.MagicMock()\n",
    "- mocker.patch(\"dependency name with module name\") returns a MagicMock object\n",
    "- During testing, MagicMock() object can be programmed to behave as a bug-free component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_list_bug_free(row):\n",
    "    # Dictionary with correct results for our desired behavior\n",
    "    # This is like a row_to_list in dict form\n",
    "    return_values = {\n",
    "        '1,801\\t201,411\\n':[\"1,801\", \"201,411\"],\n",
    "        '1,7675,112\\n': None\n",
    "    }\n",
    "    return return_values[row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass mocker as an argument\n",
    "# In this case, we want to test preprocess, and not functions\n",
    "# that compose it like row_to_list() and convert_to_int()\n",
    "def test_on_raw_data(raw_and_clean_file,\n",
    "                     mocker):\n",
    "    raw_path, clean_path = raw_and_clean_file\n",
    "    \n",
    "    # works like a bug-free replacement of row_to_list\n",
    "    row_to_list_mock = mocker.patch(\n",
    "        \"data.preprocessing_helpers.row_to_list\")\n",
    "    row_to_list_mock.side_effect = row_to_list_bug_free\n",
    "    \n",
    "    # From here on, when we call preprocess (the function being tested)\n",
    "    # The bug free version of row_to_list will be used\n",
    "    preprocess(raw_path, clean_path)\n",
    "    \n",
    "    first_line = lines[0]\n",
    "    assert first_line == '1801\\t20'\n",
    "    \n",
    "    second_line = lines[1]\n",
    "    assert second_line == '2002\\t333'    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demystifying the Patch Function (Lisa Roach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MagicMock  \n",
    "\n",
    "**Target**: 'package.module.ClassName'    \n",
    "**When should you mock**? When you don't want to actually an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my_module.py has two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "    x = db_write()\n",
    "    return x\n",
    "\n",
    "def db_write():\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to test foo without calling db_write, so we mock db_write. The test.py file would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_module\n",
    "\n",
    "@patch('my_module.db_write')\n",
    "def test_foo(self, mock_write):\n",
    "    x = my_module.foo()\n",
    "    self.assertEqual(x, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are basically replacing db_write with a MagicMock when testing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
